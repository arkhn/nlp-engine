{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Activation, LSTM, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "class RNNClassifier:\n",
    "    def __init__(self, max_len_sequence, num_classes, name_classes, model_type):\n",
    "        self.max_len_sequence = max_len_sequence\n",
    "        self.num_classes = num_classes\n",
    "        self.name_classes = name_classes\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def built_dict_character(self, corpus):\n",
    "        tokenizer = Tokenizer(char_level=True, lower=False)\n",
    "        tokenizer.fit_on_texts(corpus)\n",
    "        #Unknow character is n+1\n",
    "        self.vocab_size = len(tokenizer.word_index)+1\n",
    "        print(\"Vocabulary size is: {}\".format(self.vocab_size))\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def str_to_matrix(self, X_str):\n",
    "        X_matrix = np.zeros((len(X_str), self.max_len_sequence, self.vocab_size))\n",
    "        for i in np.arange(len(X_str)):\n",
    "            sequence_i = self.tokenizer.texts_to_matrix(X_str[i])\n",
    "            #X_matrix[i, : np.min(sequence_i.shape[0],self.max_len_sequence), :] = sequence_i[: np.min(sequence_i.shape[0],self.max_len_sequence), : ]\n",
    "            X_matrix[i, : sequence_i.shape[0], :] = sequence_i[ : self.max_len_sequence,:]\n",
    "\n",
    "        return X_matrix\n",
    "\n",
    "    def buil_model(self, use_dropout=True, hidden_size=5, dropout=0.5):\n",
    "        model = Sequential()\n",
    "        # batch size, number of time steps, hidden size)\n",
    "        #model.add(Embedding(input_dim=self.vocab_size, output_dim=hidden_size, input_length=self.max_len_sequence))\n",
    "        model.add(LSTM(hidden_size, input_shape=(self.max_len_sequence, self.vocab_size), return_sequences=False))\n",
    "        #model.add(LSTM(hidden_size, return_sequences=True))\n",
    "        if use_dropout:\n",
    "            model.add(Dropout(dropout))\n",
    "        #model.add(TimeDistributed(Dense(self.vocab_size)))\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "        #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        print(model.summary())\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X_train, Y_train, epochs=500, batch_size=1):\n",
    "        X_train = self.str_to_matrix(X_train)\n",
    "        self.model.fit(X_train,Y_train, epochs=epochs, batch_size=batch_size)\n",
    "  \n",
    "    def predict(self, X_test):\n",
    "        X_test = self.str_to_matrix(X_test)\n",
    "        return self.model.predict(X_test)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation Data\n",
    "path = \"/Users/denisdo/Desktop/mimic/mimic_database/\"\n",
    "adress = pd.read_csv(path + \"address.csv\")\n",
    "patient = pd.read_csv(path + \"PATIENTS.csv\")\n",
    "name = pd.read_csv(path + \"name.csv\")\n",
    "firstname = pd.read_csv(path + \"firstname.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = 1000\n",
    "X = pd.DataFrame( columns = ['name', 'firstname', 'adress', 'date'])\n",
    "X.name = name.name.iloc[:num]\n",
    "X.firstname = firstname.firstname.iloc[:num]\n",
    "X.adress = adress.road.iloc[:num]\n",
    "X.date = patient.DOB.iloc[:num]\n",
    "X.sample(10)\n",
    "Y= list(X.columns.values)*num\n",
    "X=list(X.values.reshape(1, 4*num)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['AABI',\n",
       "  'A',\n",
       "  'Lotissement Bellevue',\n",
       "  '2094-03-05 00:00:00',\n",
       "  'AABID',\n",
       "  'AADAM',\n",
       "  'Lotissement Les Muriers',\n",
       "  '2090-06-05 00:00:00',\n",
       "  'AALBERG',\n",
       "  'AADEL',\n",
       "  'Chemin des Abbéanches',\n",
       "  '2038-09-03 00:00:00',\n",
       "  'AAMARA',\n",
       "  'AADIL',\n",
       "  'Rue Aguétant',\n",
       "  '2075-09-21 00:00:00',\n",
       "  'AARAB',\n",
       "  'AAKASH',\n",
       "  'Rue Aimé Poncet',\n",
       "  '2114-06-20 00:00:00',\n",
       "  'AARNINK',\n",
       "  'AALIA',\n",
       "  'Rue Aimé Poncet',\n",
       "  '1895-05-17 00:00:00',\n",
       "  'AARON',\n",
       "  'AALIYA',\n",
       "  'Rue Alexandre Bérard',\n",
       "  '2108-01-15 00:00:00',\n",
       "  'AARRAS',\n",
       "  'AALIYAH',\n",
       "  'Rue Alexandre Bérard',\n",
       "  '2061-04-10 00:00:00',\n",
       "  'AATAR',\n",
       "  'AALYA',\n",
       "  'Rue Alexandre Bérard',\n",
       "  '2050-03-29 00:00:00',\n",
       "  'AATIF',\n",
       "  'AALYAH',\n",
       "  'Rue Alexandre Bérard',\n",
       "  '2051-04-21 00:00:00',\n",
       "  'AATZ',\n",
       "  'AANOR',\n",
       "  'Rue Alexandre Bérard',\n",
       "  '2053-04-13 00:00:00',\n",
       "  'AAZIZ',\n",
       "  'AARICIA',\n",
       "  'Rue Alexandre Bérard',\n",
       "  '1885-03-24 00:00:00',\n",
       "  'ABA',\n",
       "  'AARON',\n",
       "  'Rue Alexandre Bérard',\n",
       "  '2056-01-27 00:00:00',\n",
       "  'ABABOU',\n",
       "  'AARONE',\n",
       "  'Rue Alexandre Bérard',\n",
       "  '2061-10-23 00:00:00',\n",
       "  'ABABSA',\n",
       "  'AARONN',\n",
       "  'Rue Alexandre Bérard',\n",
       "  '2076-05-06 00:00:00',\n",
       "  'ABACHIN',\n",
       "  'AAROUN',\n",
       "  'Rue Alfred Rocheray',\n",
       "  '2109-04-07 00:00:00',\n",
       "  'ABAD',\n",
       "  'AAYA',\n",
       "  'Rue Alfred Rocheray',\n",
       "  '2071-02-11 00:00:00',\n",
       "  'ABADA',\n",
       "  'AAYAN',\n",
       "  'Rue Amédée Bonnet',\n",
       "  '2061-03-25 00:00:00',\n",
       "  'ABADE',\n",
       "  'AAZIZ',\n",
       "  'Rue Amédée Bonnet',\n",
       "  '2141-03-15 00:00:00',\n",
       "  'ABADI',\n",
       "  'AB',\n",
       "  'Rue Antoine Buy',\n",
       "  '2046-02-27 00:00:00',\n",
       "  'ABADIA',\n",
       "  'AB-DEL',\n",
       "  'Rue Antoine Vittet',\n",
       "  '2081-01-03 00:00:00',\n",
       "  'ABADIANO',\n",
       "  'ABABACAR',\n",
       "  'Rue des Apôtres',\n",
       "  '2031-05-19 00:00:00',\n",
       "  'ABADIAS',\n",
       "  'ABAKAR',\n",
       "  'Rue des Apôtres',\n",
       "  '2058-04-23 00:00:00',\n",
       "  'ABADIE',\n",
       "  'ABAS',\n",
       "  'Rue des Apôtres',\n",
       "  '2111-07-18 00:00:00',\n",
       "  'ABADLI',\n",
       "  'ABASS',\n",
       "  'Rue des Arènes',\n",
       "  '2101-06-10 00:00:00'],\n",
       " ['name',\n",
       "  'firstname',\n",
       "  'adress',\n",
       "  'date',\n",
       "  'name',\n",
       "  'firstname',\n",
       "  'adress',\n",
       "  'date',\n",
       "  'name',\n",
       "  'firstname'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X[:100], Y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6d0510149664>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNNClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'firstname'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'adress'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lstm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt_dict_character\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuil_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-565223dca7b6>\u001b[0m in \u001b[0;36mbuilt_dict_character\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuilt_dict_character\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m#Unknow character is n+1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    221\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                                             self.split)\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "x_train = X[:10]\n",
    "y_train = Y[:10]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_y_train = encoder.transform(y_train)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = to_categorical(encoded_y_train)\n",
    "y_train = dummy_y\n",
    "\n",
    "\n",
    "rnn = RNNClassifier(5, 4, ['name','firstname','adress','date'], 'lstm')\n",
    "rnn.built_dict_character(X)\n",
    "rnn.buil_model()\n",
    "rnn.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-55b80bd48432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt_dict_character\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m498\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-565223dca7b6>\u001b[0m in \u001b[0;36mbuilt_dict_character\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuilt_dict_character\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m#Unknow character is n+1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    221\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                                             self.split)\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "rnn.built_dict_character(X[498:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Avenue Roger Salengro', nan]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[498:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AABI',\n",
       " 'A',\n",
       " 'Lotissement Bellevue',\n",
       " '2094-03-05 00:00:00',\n",
       " 'AABID',\n",
       " 'AADAM',\n",
       " 'Lotissement Les Muriers',\n",
       " '2090-06-05 00:00:00',\n",
       " 'AALBERG',\n",
       " 'AADEL']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00382465,  0.00110351,  0.00360623,  0.99146563],\n",
       "       [ 0.00169077,  0.00619026,  0.98678583,  0.00533322],\n",
       "       [ 0.98192775,  0.0021244 ,  0.00483248,  0.01111524],\n",
       "       [ 0.00267445,  0.97514707,  0.01687247,  0.00530595],\n",
       "       [ 0.00364958,  0.00109666,  0.00348187,  0.99177188],\n",
       "       [ 0.00133675,  0.00485667,  0.98962909,  0.00417743],\n",
       "       [ 0.98192775,  0.0021244 ,  0.00483248,  0.01111524],\n",
       "       [ 0.0027628 ,  0.97499633,  0.01643029,  0.00581061],\n",
       "       [ 0.00444639,  0.00104607,  0.00404386,  0.99046361],\n",
       "       [ 0.001306  ,  0.00520222,  0.9894008 ,  0.0040909 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "       ..., \n",
       "       [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.str_to_matrix(x_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
